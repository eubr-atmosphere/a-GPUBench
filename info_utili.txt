Da target machine: su target machine: 
python3.7 -m pipenv run ./vm_scripts/launch_local_experiment.py -a tf --parameters-list app/tf/confs/local_ex -o ./tf_out --mail dispoto97@gmail.com --repetitions 1


Da host machine: host: 
python3.7 -m pipenv run ./launch_experiment.py -p apps/tf/confs/local_ex --provider local --a tf -c configurations/default.ini -d -o ./tf_out --mail <INDIRIZZO_MAIL>

Lanciato da tf.py, imagenet_command:
python3 /home/lollo/Desktop/polimi/swe_project/a-GPUBench-master/apps/tf/slim/train_image_classifier.py --log_every_n_steps=1 --model_name=alexnet_v2 --max_number_of_steps=1 --batch_size=64 --momentum=0.9 --num_readers=4 --num_preprocessing_threads=4 --dataset_dir=/home/lollo/tmp/flowers/5 --num_classes=5 --save_summaries_secs=0 --save_interval_secs=0 --optimizer=momentum

Lanciato da tf.py, imagenet_command, per flowers, con nostre modifiche:
python3 /home/lollo/Desktop/polimi/swe_project/a-GPUBench-master/apps/tf/slim/train_image_classifier.py --log_every_n_steps=1 --model_name=alexnet_v2 --max_number_of_steps=1 --batch_size=64 --momentum=0.9 --num_readers=4 --num_preprocessing_threads=4 --dataset_name=flowers --dataset_dir=/home/lollo/tmp/flowers/5 --num_classes=5 --save_summaries_secs=0 --save_interval_secs=0 --optimizer=momentum 

In order to execute into docker container:
 - move out of the folder Dockerfile
 - docker build -t gpubench-docker
 - docker volume create gpuenv
 - run launch_experiment:
    python3.7 -m pipenv run ./launch_experiment.py -p apps/tf/confs/local_ex --provider inhouse  --a tf -c configurations/default.ini -d -o ./tf_out

Pay attention to disable email since there are errors.
The output of the training session is stored into the folder.
Pay attention that local_test is the correct one

Pay attention to fill raw-data with files 


DISCLAMER:
When executing VQA model, add in the same directory of the outputted tfrecords file the zip with correct dataset,
in order to start unzipping and so on.
After the end of the process, move inside the directory 58 the whole ds and start training.

In order to build tfrecords of dataset, move inside the directory with VQA_dataset folder and use that folder as directory to script that build dataset.


Setup Dataset on AIRLab Server

Download and convert script must be runned outside the container building in order to add more dataset and keep indipendent the two
Script:
NV_GPU=2 nvidia-docker run --user 1060:1060 -v ~/storage/data/:/data dispoto /bin/bash -c ' python3 ./app/apps/tf/slim/download_and_convert_data.py --dataset_name=flowers --dataset_dir="/data/dataset/flowers/" && mkdir /data/dataset/flowers/5 &&  cp /data/dataset/flowers/*.tfrecord /data/dataset/flowers/5'

RUN on AIRLAB machine

python3.7 ./launch_experiment.py -p apps/tf/confs/local_ex --provider inhouse --a tf -c configurations/default.ini  --output ./data/output --profile GPU

RUN ON CONTAINER 
docker run --rm --network=host --name client -v ~/Desktop/ResearchProject/container-data:/container-data test/run-enviroment /bin/bash -c 'cd /container-data/GPUPorting && eval $(ssh-agent -s) && ssh-add keys/id_rsa && python3 ./launch_experiment.py -p apps/tf/confs/local_ex --provider local --a tf -c configurations/default.ini  --output ./data/output --profile GPU'


Pay attention that on AIRLab server there is the main training script with error.
When training ends, you need to deploy the version on this repo.

Inside the directory env there is the Dockerfile of the target machine